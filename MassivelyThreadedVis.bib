%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for kmorel at 2013-09-05 14:38:36 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@article{Moreland2013:TVCG,
	Abstract = {The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over twenty years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.},
	Annote = {A survey of research pertaining to visualization pipelines, the libraries that implement them, and the tools that use them.},
	Author = {Kenneth Moreland},
	Date-Added = {2013-09-05 20:37:13 +0000},
	Date-Modified = {2013-09-05 20:37:13 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {March},
	Note = {{DOI}~10.1109/TVCG.2012.133},
	Number = {3},
	Pages = {367--378},
	Title = {A Survey of Visualization Pipelines},
	Volume = {19},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2012.133}}

@article{Biddiscombe2007,
	Annote = {Adds extensions to the visualization pipeline that allow filters to report and control time.},
	Author = {John Biddiscombe and Berk Geveci and Ken Martin and Kenneth Moreland and David Thompson},
	Date-Added = {2013-09-05 20:36:42 +0000},
	Date-Modified = {2013-09-05 20:36:42 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {November/December},
	Note = {{DOI}~10.1109/TVCG.2007.70600},
	Number = {6},
	Pages = {1376--1383},
	Title = {Time Dependent Processing in a Parallel Pipeline Architecture},
	Volume = {13},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2007.70600}}

@article{Zhou2008,
	Abstract = {We present an algorithm for constructing kd-trees on GPUs. This algorithm achieves real-time performance by exploiting the GPU's streaming architecture at all stages of kd-tree construction. Unlike previous parallel kd-tree algorithms, our method builds tree nodes completely in BFS (breadth-first search) order. We also develop a special strategy for large nodes at upper tree levels so as to further exploit the fine-grained parallelism of GPUs. For these nodes, we parallelize the computation over all geometric primitives instead of nodes at each level. Finally, in order to maintain kd-tree quality, we introduce novel schemes for fast evaluation of node split costs.

As far as we know, ours is the first real-time kd-tree algorithm on the GPU. The kd-trees built by our algorithm are of comparable quality as those constructed by off-line CPU algorithms. In terms of speed, our algorithm is significantly faster than well-optimized single-core CPU algorithms and competitive with multi-core CPU algorithms. Our algorithm provides a general way for handling dynamic scenes on the GPU. We demonstrate the potential of our algorithm in applications involving dynamic scenes, including GPU ray tracing, interactive photon mapping, and point cloud modeling.},
	Annote = {Builds and uses a kd-tree on a GPU. The paper is very detailed and uses an algorithm more complicated than that, say, used by PISTON. The kd-tree is built using a surface area heuristic as the metric for making splits in the tree.},
	Author = {Kun Zhou and Qiming Hou and Rui Wang and Baining Guo},
	Date-Added = {2013-09-05 17:02:29 +0000},
	Date-Modified = {2013-09-05 17:02:29 +0000},
	Journal = {ACM Transactions on Graphics},
	Month = {December},
	Note = {{DOI}~10.1145/1409060.1409079},
	Number = {5},
	Title = {Real-Time KD-Tree Construction on Graphics Hardware},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1409060.1409079}}

@inproceedings{Foley2005,
	Abstract = {Modern graphics hardware architectures excel at compute-intensive tasks such as ray-triangle intersection, making them attractive target platforms for raytracing. To date, most GPU-based raytracers have relied upon uniform grid acceleration structures. In contrast, the kd-tree has gained widespread use in CPU-based raytracers and is regarded as the best general-purpose acceleration structure. We demonstrate two kd-tree traversal algorithms suitable for GPU implementation and integrate them into a streaming raytracer. We show that for scenes with many objects at different scales, our kd-tree algorithms are up to 8 times faster than a uniform grid. In addition, we identify load balancing and input data recirculation as two fundamental sources of inefficiency when raytracing on current graphics hardware.},
	Annote = {Describes a kd-tree search structure used on a GPU. The kd-tree still needs to be built on a CPU. The tree is traversed with what is known as a kd-restart algorithm.

The kd-tree is used for ray casting.
},
	Author = {Tim Foley and Jeremy Sugerman},
	Booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware (HWWS '05)},
	Date-Added = {2013-09-05 16:57:54 +0000},
	Date-Modified = {2013-09-05 16:57:54 +0000},
	Note = {{DOI}~10.1145/1071866.1071869},
	Pages = {15--22},
	Title = {{KD}-Tree Acceleration Structures for a {GPU} Raytracer},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1071866.1071869}}

@article{Zhou2010,
	Abstract = {We present the first parallel surface reconstruction algorithm that runs entirely on the GPU. Like existing implicit surface reconstruction methods, our algorithm first builds an octree for the given set of oriented points, then computes an implicit function over the space of the octree, and finally extracts an isosurface as a watertight triangle mesh. A key component of our algorithm is a novel technique for octree construction on the GPU. This technique builds octrees in real time and uses level-order traversals to exploit the fine-grained parallelism of the GPU. Moreover, the technique produces octrees that provide fast access to the neighborhood information of each octree node, which is critical for fast GPU surface reconstruction. With an octree so constructed, our GPU algorithm performs Poisson surface reconstruction, which produces high-quality surfaces through a global optimization. Given a set of 500 K points, our algorithm runs at the rate of about five frames per second, which is over two orders of magnitude faster than previous CPU algorithms. To demonstrate the potential of our algorithm, we propose a user-guided surface reconstruction technique which reduces the topological ambiguity and improves reconstruction results for imperfect scan data. We also show how to use our algorithm to perform on-the-fly conversion from dynamic point clouds to surfaces as well as to reconstruct fluid surfaces for real-time fluid simulation.},
	Annote = {Describes an Octree spatial search structure that can be entirely build and used on a GPU. The Octree is built in a bottom-up fashion where primitives are placed in the most refined grid considered, and then spares areas of the grid are combined to implement the coarser levels of the tree.

This Octree spatial search structure is used for reconstructing surfaces from point clouds.},
	Author = {Kun Zhou and Minmin Gong and Xin Huang and Baining Guo},
	Date-Added = {2013-09-05 16:35:49 +0000},
	Date-Modified = {2013-09-05 16:35:49 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {May},
	Note = {{DOI}~10.1109/TVCG.2010.75},
	Number = {5},
	Pages = {669--681},
	Title = {Data-Parallel Octrees for Surface Reconstruction},
	Volume = {17},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2010.75}}

@article{Kalojanov2011,
	Abstract = {We investigate the use of two-level nested grids as acceleration structure for ray tracing of dynamic scenes. We propose a massively parallel, sort-based construction algorithm and show that the two-level grid is one of the structures that is fastest to construct on modern graphics processors. The structure handles non-uniform primitive distributions more robustly than the uniform grid and its traversal performance is comparable to those of other high quality acceleration structures used for dynamic scenes. We propose a cost model to determine the grid resolution and improve SIMD utilization during ray-triangle intersection by employing a hybrid packetization strategy. The build times and ray traversal acceleration provide overall rendering performance superior to previous approaches for real time rendering of animated scenes on GPUs.},
	Annote = {An extension to [Kalojanov2009]. This also builds and uses a spatial search structure on GPUs. It is a two-level hiearchy of uniform grids. The first grid is a coarse partitioning of space. In each cell of this coarse partitioning a second level grid is created. The size of this grid is proportional to the number of primitives in that level.},
	Author = {Javor Kalojanov and Markus Billeter and Philipp Slusallek},
	Date-Added = {2013-09-05 16:24:22 +0000},
	Date-Modified = {2013-09-05 16:24:22 +0000},
	Journal = {Computer Graphics Forum},
	Month = {April},
	Note = {{DOI}~10.1111/j.1467-8659.2011.01862.x},
	Number = {2},
	Pages = {307--314},
	Title = {Two-Level Grids for Ray Tracing on {GPU}s},
	Volume = {30},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1111/j.1467-8659.2011.01862.x}}

@inproceedings{Kalojanov2009,
	Abstract = {We present a fast, parallel GPU algorithm for construction of uniform grids for ray tracing, which we implement in CUDA. The algorithm performance does not depend on the primitive distribution, because we reduce the problem to sorting pairs of primitives and cell indices. Our implementation is able to take full advantage of the parallel architecture of the GPU, and construction speed is faster than CPU algorithms running on multiple cores. Its scalability and robustness make it superior to alternative approaches, especially for scenes with complex primitive distributions.},
	Annote = {A spatial search structure that it built and executed on a GPU. The search structure is a uniform grid that points to offsets in a list of triangles.

The search structure was specifically designed for ray casting.},
	Author = {Javor Kalojanov and Philipp Slusallek},
	Booktitle = {Proceedings of the Conference on High Performance Graphics},
	Date-Added = {2013-09-05 16:15:02 +0000},
	Date-Modified = {2013-09-05 16:15:02 +0000},
	Note = {{DOI}~10.1145/1572769.1572773},
	Pages = {23--28},
	Title = {A Parallel Algorithm for Construction of Uniform Grids},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1572769.1572773}}

@inproceedings{Rubel2008,
	Annote = {Specifically uses VisIt for query-based visualization.},
	Author = {Oliver {R\"{u}bel} and Prabhat and Kesheng Wu and Hank Childs and Jeremy Meredith and Cameron G.R. Geddes and Estelle Cormier-Michel and Sean Ahern and Gunther H. Weber and Peter Messmer and Hans Hagen and Bernd Hamann and E. Wes Bethel},
	Booktitle = {Proceedings of the 2008 ACM/IEEE Conference on Supercomputing},
	Date-Added = {2013-09-05 15:57:16 +0000},
	Date-Modified = {2013-09-05 15:57:16 +0000},
	Month = {November},
	Title = {High Performance Multivariate Visual Data Exploration for Extremely Large Data},
	Year = {2008}}

@article{Gosink2008,
	Annote = {Follow on to Stockinger2005 to extend to AMR data sets.},
	Author = {Luke J. Gosink and John C. Anderson and E. Wes Bethel and Kenneth I. Joy},
	Date-Added = {2013-09-05 15:56:50 +0000},
	Date-Modified = {2013-09-05 15:56:50 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {November/December},
	Number = {6},
	Pages = {1715--1722},
	Title = {Query-Driven Visualization of Time-Varying Adaptive Mesh Refinement Data},
	Volume = {14},
	Year = {2008}}

@article{Johnson2009,
	Annote = {A predicate-based language for query-based visualization.},
	Author = {C. Ryan Johnson and Jian Huang},
	Date-Added = {2013-09-05 15:56:26 +0000},
	Date-Modified = {2013-09-05 15:56:26 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {September/October},
	Note = {{DOI}~10.1109/TVCG.2009.25},
	Number = {5},
	Title = {Distribution-Driven Visualization of Volume Data},
	Volume = {15},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2009.25}}

@article{Glatter2008,
	Annote = {An expression language for query-based visualization based on file globbing.},
	Author = {Markus Glatter and Jian Huang and Sean Ahern and Jamison Daniel and Aidong Lu},
	Date-Added = {2013-09-05 15:56:18 +0000},
	Date-Modified = {2013-09-05 15:56:18 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Month = {November/December},
	Note = {{DOI}~10.1109/TVCG.2008.184},
	Number = {6},
	Pages = {1467--1474},
	Title = {Visualizing Temporal Patterns in Large Multivariate Data using Textual Pattern Matching},
	Volume = {14},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TVCG.2008.184}}

@techreport{CGNS,
	Annote = {CFD General Notation System (CGNS) is an industry standard for representing topologies.},
	Author = {Christopher L. Rumsey and Diane M. A. Poirier and Robert H. Bush and Charles E. Towne},
	Date-Added = {2013-09-04 22:51:23 +0000},
	Date-Modified = {2013-09-04 22:51:23 +0000},
	Institution = {NASA},
	Month = {October},
	Number = {TM-2001-211236},
	Title = {A User's Guide to CGNS},
	Year = {2001}}

@inproceedings{Levy2001,
	Abstract = {We present the Circular Incident Edge Lists (CIEL), a new data structure and a high-performance algorithm for generating a series of iso-surfaces in a highly unstructured grid. Slicing-based volume rendering is also considered. The CIEL data structure represents all the combinatorial information of the grid, making it possible to optimize the classical propagation from local minima paradigm. The usual geometric structures are replaced by a more efficient combinatorial structure. An active edges list is maintained, and iteratively propagated from an iso-surface to the next one in a very efficient way. The intersected cells incident to each active edge are retrieved, and the intersection polygons are generated by circulating around their facets. This latter feature enables arbitrary irregular cells to be treated, such as those encountered in certain computational fluid dynamics (CFD) simulations. Since the CIEL data structure solely depends on the connections between the cells, it is possible to take into account dynamic changes in the geometry of the mesh and in property values, which only requires the sorted extrema list to be updated. Experiments have shown that our approach is significantly faster than classical methods. The major drawback of our method is its memory consumption, higher than most classical methods. However, experimental results show that it stays within a practical range.},
	Annote = {Describes circular incident edge lists (CIEL) to represent polyhedral meshes. Circular incident edge lists are represented almost wholly by half edges. Each half-edge belongs to a particular face of a particular cell. Each half-edge has several links to capture the polyhedra topology:

* A link to the next vertex in the face of the cell (equivalent to the adjacent edge link in a half-edge data structure).
* A link to the half edge of the adjacent face in the same cell (equivalent to the adjacent polygon in a half-edge data structure).
* A link to the corresponding half-edge in the adjacent cell along the face (which will be pointing in the opposite direction).
* A linked list of all half-edges attached to a vertex.},
	Author = {Bruno L\'{e}vy and Guillaume Caumon and St\'{e}phane Conreaux and Xavier Cavin},
	Booktitle = {Proceedings of IEEE Visualization},
	Date-Added = {2013-09-04 22:44:31 +0000},
	Date-Modified = {2013-09-04 22:44:31 +0000},
	Month = {October},
	Pages = {191--198},
	Title = {Circular Incident Edge Lists: a Data Structure for Rendering Complex Unstructured Grids},
	Year = {2001}}

@inproceedings{Kettner1998,
	Abstract = {Design solutions for a program library are presented for combinato- 
rial data structures in computational geometry, such as planar maps 
and polyhedral surfaces. Design issues considered are genericity, 
flcsibility, time and space efficiency, and ease-of-use. We focus 
on topological aspects of polyhedral surfaces. Edge-based repre- 
w%ations for polyhedrons are evaluated with respect to the design 
goals. A design for polyhedral surfaces in a halfedge data structure 
is developed following the generic programming paradigm known 
from the Standard Template Library STL for C++. Connections 
arc shown to planar maps and face-based structures managing holes 
in facets.},
	Annote = {Describes half-edge structures for representing polygonal meshes. In a conformal polygon mesh, each edge is shared by (at most) two polygons. In the half-edge structure, each edge is represented by two half edges, each associated with one of the two adjacent polygons. A polygon is represented as the cycle of half edges around it. Each half-edge points to the next half edge and the adjacent half edge, which is pointing in the opposite direction.},
	Author = {Lutz Kettner},
	Booktitle = {Proceedings of the Fourteenth ACM Symposium on Computational Geometry},
	Date-Added = {2013-09-04 22:31:21 +0000},
	Date-Modified = {2013-09-04 22:31:21 +0000},
	Note = {{DOI}~10.1145/276884.276901},
	Pages = {146--154},
	Title = {Designing a Data Structure for Polyhedral Surfaces},
	Year = {1998}}

@inproceedings{Alumbaugh2005,
	Abstract = {In this paper, we present simple and efficient array-based mesh data structures, including a compact representation of the half-edge data structure for surface meshes, and its generalization --a half-face data structure-- for volume meshes. These array-based structures provide comprehensive and efficient support for querying incidence, adjacency, and boundary classification, but require substantially less memory than pointer-based mesh representations. In addition, they are easy to implement in traditional programming languages (such as in C or Fortran 90) and convenient to exchange across different software packages or different storage media. In a parallel setting, they also support partitioned meshes and hence are particularly appealing for large-scale scientific and engineering applications. We demonstrate the construction and usage of these data structures for various operations, and compare their space and time complexities with alternative structures.},
	Annote = {Describes a cellular data structure (sometimes abbreviated CDS). It is based on the observation that each polyhedron in a linear mesh is itself a manifold polygon mesh. Thus, each polyhedron is represented with a half-edge structure. The global mesh topology is represented by half-faces (also sometimes called half-facets). Each cell has a circular linked list of half-faces. As the name implies, there are two half-faces for each face in the mesh, one for each incident cell. Each half-face points to the next half-face in the cell's list, the half-face for the adjacent cell, and a half edge for the face in the half-edge structure for this cell.},
	Author = {Tyler J Alumbaugh and Xiangmin Jiao},
	Booktitle = {Proceedings, 14th International Meshing Roundtable},
	Date-Added = {2013-09-04 22:23:32 +0000},
	Date-Modified = {2013-09-04 22:23:32 +0000},
	Month = {September},
	Pages = {485--504},
	Title = {Compact Array-Based Mesh Data Structures},
	Year = {2005}}

@misc{Bell2010,
	Annote = {A presentation containing several example programs in thrust.  One important one for us (and does not have a lot of other examples) is an implementation of vertex-weld.},
	Author = {Bell, Nathan},
	Booktitle = {GPU Technology Conference},
	Date-Added = {2013-09-04 20:02:08 +0000},
	Date-Modified = {2013-09-04 20:02:08 +0000},
	File = {:home/bob/Desktop/Old Ubuntu FS/home/bob/Papers/GPUCoincidentPoints/References/GTC 2010 (Part 1) - High Productivity Development.pdf:pdf},
	Title = {High-Productivity {CUDA} Development with the Thrust Template Library},
	Year = {2010}}

@book{VTKUsersGuide,
	Annote = {User's guide for VTK.},
	Date-Added = {2013-09-04 19:58:43 +0000},
	Date-Modified = {2013-09-05 15:30:03 +0000},
	Edition = {11th},
	Note = {{ISBN} 978-1-930934-23-8},
	Publisher = {Kitware Inc.},
	Title = {The VTK User's Guide},
	Year = {2010}}

@book{VTK,
	Annote = {Dude, it's VTK.},
	Author = {Will Schroeder and Ken Martin and Bill Lorensen},
	Date-Added = {2013-09-04 19:54:49 +0000},
	Date-Modified = {2013-09-04 19:54:49 +0000},
	Edition = {Fourth},
	Note = {{ISBN} 1-930934-19-X},
	Publisher = {Kitware Inc.},
	Title = {The Visualization Toolkit: An Object Oriented Approach to {3D} Graphics},
	Year = {2004}}

@article{Childs2013,
	Abstract = {As the visualization research community reorients its software to address up-coming challenges, it must successfully deal with diverse processor architectures, distributed systems, various data sources, massive parallelism, multiple input and output devices, and interactivity.
},
	Annote = {This article gives a high level overview of the challenges of large-scale HPC visualization in the exascale/extreme scale from a technical standpoint.  The issues raised are: Massive Parallelization, Processor Architectures and Programming Models, Application Architecture and Data Management, Data Models, Rendering, and Interaction.},
	Author = {Hank Childs and Berk Geveci and Will Schroeder and Jeremy Meredith and Kenneth Moreland and Christopher Sewell and Torsten Kuhlen and E. Wes Bethel},
	Date-Added = {2013-09-04 15:38:35 +0000},
	Date-Modified = {2013-09-04 15:38:35 +0000},
	Journal = {IEEE Computer},
	Month = {May},
	Note = {{DOI}~10.1109/MC.2013.179},
	Number = {5},
	Pages = {34--42},
	Title = {Research Challenges for Visualization Software},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MC.2013.179}}

@inproceedings{Meredith2012,
	Abstract = {Analysis and visualization of the data generated by scientific simulation codes is a key step in enabling science from computation. However, a number of challenges lie along the current hardware and software paths to scientific discovery. First, only advanced parallelism techniques can take full advantage of the unprecedented scale of coming machines. In addition, as computational improvements outpace those of I/O, more data will be discarded and I/O-heavy analysis will suffer. Furthermore, the limited memory environment, particularly in the context of in situ analysis which can sidestep some I/O limitations, will require efficiency of both algorithms and infrastructure. Finally, advanced simulation codes with complex data models require commensurate data models in analysis tools. However, community visualization and analysis tools designed for parallelism and large data fall short in a number of these areas. In this paper, we describe EAVL, a new library with infrastructure and algorithms designed to address these critical needs for current and future generations of scientific software and hardware. We show results from EAVL demonstrating the strengths of its robust data model, advanced parallelism, and efficiency.},
	Annote = {Another EAVL paper with a bit more technical content than the GPGPU-5 reference. Talks a bit more about how the data model works and provides more examples.

Specifically talkes about dimentionality reduction, high order interpolation, scalar warp (called elevation in the paper) and threshold using advanced structures.
},
	Author = {Jeremy S. Meredith and Sean Ahern and Dave Pugmire and Robert Sisneros},
	Booktitle = {Eurographics Symposium on Parallel Graphics and Visualization (EGPGV)},
	Date-Added = {2013-09-03 23:39:47 +0000},
	Date-Modified = {2013-09-03 23:39:47 +0000},
	Note = {{DOI}~10.2312/EGPGV/EGPGV12/021-030},
	Pages = {21--30},
	Title = {{EAVL}: The Extreme-scale Analysis and Visualization Library},
	Year = {2012}}

@inproceedings{Maynard2013,
	Abstract = {As the HPC community starts focusing its efforts towards exascale, it becomes clear that we are looking at machines with a billion way concurrency. Although parallel computing has been at the core of the performance gains achieved until now, scaling over 1,000 times the current concurrency can be challenging. As discussed in this paper, even the smallest memory access and synchronization overheads can cause major bottlenecks at this scale. As we develop new software and adapt existing algorithms for exascale, we need to be cognizant of such pitfalls. In this paper, we document our experience with optimizing a fairly common and parallelizable visualization algorithm, threshold of cells based on scalar values, for such highly concurrent architectures. Our experiments help us identify design patterns that can be generalized for other visualization algorithms as well. We discuss our implementation within the Dax toolkit, which is a framework for data analysis and visualization at extreme scale. The Dax toolkit employs the patterns discussed here within the framework's scaffolding to make it easier for algorithm developers to write algorithms without having to worry about such scaling issues.},
	Annote = {A reporting on some of the optimizations we did for threshold in Dax.},
	Author = {Robert Maynard and Kenneth Moreland and Utkarsh Ayachit and Berk Geveci and Kwan-Liu Ma},
	Booktitle = {Visualization and Data Analysis 2013, Proceedings of SPIE-IS\&T Electronic Imaging},
	Date-Added = {2013-09-03 23:23:47 +0000},
	Date-Modified = {2013-09-03 23:23:47 +0000},
	Month = {February},
	Title = {Optimizing Threshold for Extreme Scale Analysis},
	Year = {2013}}

@techreport{PISTON,
	Abstract = {Due to the wide variety of current and next-generation supercomputing architectures, the development of highperformance parallel visualization and analysis operators frequently requires re-writing the underlying algorithms for many different platforms. In order to facilitate portability, we have devised a framework for creating such operators that employs the data-parallel programming model. By writing the operators using only data-parallel primitives (such as scans, transforms, stream compactions, etc.), the same code may be compiled to multiple targets using architecture-specific backend implementations of these primitives. Specifically, we make use of and extend NVIDIA's Thrust library, which provides CUDA and OpenMP backends. Using this framework, we have implemented isosurface, cut surface, and threshold operators, and have achieved good parallel performance on two different architectures (multi-core CPUs and NVIDIA GPUs) using the exact same operator code. We have applied these operators to several large, real scientific data sets, and have open-source released a beta version of our code base.},
	Annote = {A library providing portable implementations of visualization algorithms on multi- and many-core processors.  Uses Thrust to implement algorithms and portability.  Compares marching cubes isosurface generation to existing implementations by NVIDIA and VTK.

I believe this was presented at EGPGV 2012 and when it becomes available in the proceedings this reference should be updated.
},
	Author = {Li-Ta Lo and Chris Sewell and James Ahrens},
	Date-Added = {2013-09-03 23:18:03 +0000},
	Date-Modified = {2013-09-03 23:18:03 +0000},
	Institution = {Los Alamos National Laboratory},
	Number = {LA-UR-12-10227},
	Title = {{PISTON}: A Portable Cross-Platform Framework for Data-Parallel Visualization Operators},
	Year = {2012},
	Bdsk-Url-1 = {http://viz.lanl.gov/projects/PISTON.html}}

@inproceedings{Sewell2012,
	Abstract = {This paper surveys the four software frameworks being developed as part of the visualization pillar of the SDAV (Scalable Data Management, Analysis, and Visualization) Institute, one of the SciDAC (Scientific Discovery through Advanced Computing) Institutes established by the ASCR (Advanced Scientific Computing Research) Program of the U.S. Department of Energy. These frameworks include EAVL (Extreme-scale Analysis and Visualization Library), DAX (Data Analysis at Extreme), DIY (Do It Yourself), and PISTON. The objective of these frameworks is to facilitate the adaptation of visualization and analysis algorithms to take advantage of the available parallelism in emerging multi-core and many-core hardware architectures, in anticipation of the need for such algorithms to be run in-situ with LCF (leadership-class facilities) simulation codes on supercomputers.},
	Annote = {Describes four frameworks addressing large-scale HPC visualization problems.  These are EAVL, Dax, DIY, and PISTON.
},
	Author = {Christopher Sewell and Jeremy Meredith and Kenneth Moreland and Tom Peterka and Dave DeMarle and Li-Ta Lo and James Ahrens and Robert Maynard and Berk Geveci},
	Booktitle = {2012 SC Companion (Proceedings of the Ultrascale Visualization Workshop)},
	Date-Added = {2013-09-03 23:17:47 +0000},
	Date-Modified = {2013-09-05 15:26:04 +0000},
	Month = {November},
	Note = {{DOI}~10.1109/SC.Companion.2012.36},
	Pages = {206--214},
	Title = {The {SDAV} Software Frameworks for Visualization and Analysis on Next-Generation Multi-Core and Many-Core Architectures},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/SC.Companion.2012.36}}

@inproceedings{Moreland2011:LDAV,
	Annote = {Description of the Dax toolkit, a framework for building algorithms for GPU computers and, later, exascale computers.},
	Author = {Kenneth Moreland and Utkarsh Ayachit and Berk Geveci and Kwan-Liu Ma},
	Booktitle = {Proceedings of the IEEE Symposium on Large-Scale Data Analysis and Visualization},
	Date-Added = {2013-09-03 22:44:03 +0000},
	Date-Modified = {2013-09-03 22:44:03 +0000},
	Month = {October},
	Note = {{DOI}~10.1109/LDAV.2011.6092323},
	Pages = {97--104},
	Title = {Dax Toolkit: A Proposed Framework for Data Analysis and Visualization at Extreme Scale},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/LDAV.2011.6092323}}

@inproceedings{EAVL,
	Abstract = {The coming generation of supercomputing architectures will require fundamental changes in programming models to effectively make use of the expected million to billion way concurrency and thousand-fold reduction in per-core memory. Most current parallel analysis and visualization tools achieve scalability by partitioning the data, either spatially or temporally, and running serial computational kernels on each data partition, using message passing as needed. These techniques lack the necessary level of data parallelism to execute effectively on the underlying hardware. This paper introduces a framework that enables the expression of analysis and visualization algorithms with memory-efficient execution in a hybrid distributed and data parallel manner on both multi-core and many-core processors. We demonstrate results on scientific data using CPUs and GPUs in scalable heterogeneous systems.},
	Annote = {Provides an overview of EAVL, mult-core, many-core, GPU, exascale thinking system.  EAVL defines operations with the use of functors.  Current operations look limited to basic mapping operations that can look at topological connections.  The really unique thing about EAVL is its data model that can adapt to many topological features and can handle things like edge and face data.  It also seems like it can map pretty easily to these features.},
	Author = {Jeremy S. Meredith and Robert Sisneros and David Pugmire and Sean Ahern},
	Booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units (GPGPU-5)},
	Date-Added = {2013-09-03 22:41:45 +0000},
	Date-Modified = {2013-09-03 22:41:45 +0000},
	Month = {March},
	Note = {{DOI}~10.1145/2159430.2159432},
	Pages = {11--19},
	Title = {A Distributed Data-Parallel Framework for Analysis and Visualization Algorithm Development},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2159430.2159432}}

@book{Quinn2004,
	Annote = {A book containing mostly practical algorithms and their implementations using MPI and OpenMP.  The book also contains what I think is a nice review of some of the basic theory of parallel computing including Amdahl's law, Gustafson-Barsis's law, the Karp-Flatt metric, and the Isoefficiency metric.},
	Author = {Michael J. Quinn},
	Date-Added = {2013-09-03 21:31:50 +0000},
	Date-Modified = {2013-09-03 21:31:50 +0000},
	Note = {{ISBN}~978-0-07-282256-4},
	Publisher = {McGraw-Hill},
	Title = {Parallel Programming in C with MPI and OpenMP},
	Year = {2004}}

@book{TBB,
	Annote = {The user's guide and reference book for Intel Threading Building Blocks (TBB).  Also contains lots of practical advice on creating efficient application including notes on locking, allocations, cache-lines, and false-sharing.},
	Author = {James Reinders},
	Date-Added = {2013-09-03 21:29:31 +0000},
	Date-Modified = {2013-09-03 21:29:31 +0000},
	Month = {July},
	Note = {{ISBN}~978-0-596-51480-8},
	Publisher = {O'Reilly},
	Title = {Intel Threading Building Blocks: Outfitting {C++} for Multi-core Processor Parallelism},
	Year = {2007}}

@inbook{Thrust,
	Annote = {A portable STL-like template library for use on multi- and many-core systems.},
	Author = {Nathan Bell and Jared Hoberock},
	Chapter = {Thrust: A Productivity-Oriented Library for {CUDA}},
	Date-Added = {2013-09-03 21:29:13 +0000},
	Date-Modified = {2013-09-03 21:29:13 +0000},
	Month = {October},
	Pages = {359--371},
	Publisher = {Morgan Kaufmann},
	Title = {GPU Computing Gems, Jade Edition},
	Year = {2011}}

@book{Sanders2011,
	Annote = {A book describing how to use CUDA including a lot of practical advice.},
	Author = {Jason Sanders and Edward Kandrot},
	Date-Added = {2013-09-03 21:28:53 +0000},
	Date-Modified = {2013-09-03 21:28:53 +0000},
	Note = {{ISBN}~978-0-13-138768-3},
	Publisher = {Addison Wesley},
	Title = {{CUDA} by Example},
	Year = {2011}}

@inproceedings{Baker2010,
	Abstract = {Multicore nodes have become ubiquitous in just a few years. At the same time, writing portable parallel software for multicore nodes is extremely challenging. Widely available programming models such as OpenMP and Pthreads are not useful for devices such as graphics cards, and more flexible programming models such as RapidMind are only available commercially. OpenCL represents the first truly portable standard, but its availability is limited. In the presence of such transition, we have developed a minimal application programming interface (API) for multicore nodes that allows us to write portable parallel linear algebra software that can use any of the aforementioned programming models and any future standard models. We utilize C++ template meta-programming to enable users to write parallel kernels that can be executed on a variety of node types, including Cell, GPUs and multicore CPUs. The support for a parallel node is provided by implementing a Node object, according to the requirements specified by the API. This ability to provide custom support for particular node types gives developers a level of control not allowed by the current slate of proprietary parallel programming APIs. We demonstrate implementations of the API for a simple vector dot-product on sequential CPU, multicore CPU and GPU nodes.},
	Annote = {This is a paper from Mike Heroux and gang about the multi-core parallel technique using functors.  It makes for a vary portable system parallel system and this is demonstrated on intel threaded building blocks (TBB) and CUDA.},
	Author = {Christopher G. Baker and Michael A. Heroux and H. Carter Edwards and Alan B. Williams},
	Booktitle = {Proceedings of the 18th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
	Date-Added = {2013-09-03 20:54:28 +0000},
	Date-Modified = {2013-09-04 16:04:40 +0000},
	Month = {February},
	Note = {{DOI}~10.1109/PDP.2010.49},
	Pages = {601--606},
	Title = {A Light-weight {API} for Portable Multicore Programming},
	Year = {2010},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5452412},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/PDP.2010.49}}

@article{Lorensen1987,
	Annote = {The seminal work on the marching cubes algorithm for contour/isosurface.},
	Author = {William E. Lorensen and Harvey E. Cline},
	Date-Added = {2013-08-30 15:08:16 -0600},
	Date-Modified = {2013-08-30 15:08:16 -0600},
	Journal = {Computer Graphics (Proceedings of SIGGRAPH 87)},
	Month = {July},
	Number = {4},
	Pages = {163--169},
	Title = {Marching Cubes: A High Resolution {3D} Surface Construction Algorithm},
	Volume = {21},
	Year = {1987}}

@article{Ahrens2001,
	Annote = {One of the seminal papers on introducing parallelism and data streaming into VTK specifically and into pipelines in general.  Talks significantly about ghost cells and requirements for algorithms to work well in streaming.  Less talk about data parallelism.},
	Author = {James Ahrens and Kristi Brislawn and Ken Martin and Berk Geveci and C. Charles Law and Michael Papka},
	Date-Added = {2013-08-29 22:31:41 +0000},
	Date-Modified = {2013-08-29 22:31:41 +0000},
	Journal = {IEEE Computer Graphics and Applications},
	Month = {July/August},
	Number = {4},
	Pages = {34--41},
	Title = {Large-Scale Data Visualization Using Parallel Data Streaming},
	Volume = {21},
	Year = {2001}}

@inproceedings{Law1999,
	Annote = {Describes out-of-core streaming in VTK/visualization pipelines.  Also describes the properties an algorithm must have to enable streaming: separable, result invariant, and mappable.},
	Author = {C. Charles Law and Kenneth M. Martin and William J. Schroeder and Joshua Temkin},
	Booktitle = {Proceedings of IEEE Visualization 1999},
	Date-Added = {2013-08-29 22:31:35 +0000},
	Date-Modified = {2013-08-29 22:31:35 +0000},
	Month = {October},
	Pages = {225--232},
	Title = {A Multi-Threaded Streaming Pipeline Architecture for Large Structured Data Sets},
	Year = {1999}}

@article{Sutter2005,
	Annote = {A well cited article describing the trend of processors from faster cores every generation to more threads every generation. Argues that the "free lunch" of algorithms working better over time because processors get faster is over. The predictions of the article have generally been shown to be right.},
	Author = {Herb Sutter},
	Date-Added = {2013-08-28 22:33:53 +0000},
	Date-Modified = {2013-08-28 22:33:53 +0000},
	Journal = {Dr. Dobb's Journal},
	Number = {3},
	Title = {The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software},
	Volume = {30},
	Year = {2005}}

@book{ParaView,
	Annote = {Dude, its ParaView.},
	Author = {Utkarsh Ayachit and others},
	Date-Added = {2013-08-27 23:19:50 +0000},
	Date-Modified = {2013-08-27 23:19:50 +0000},
	Edition = {4th},
	Note = {{ISBN} 978-1-930934-24-5},
	Publisher = {Kitware Inc.},
	Title = {The {ParaView} Guide: A Parallel Visualization Application},
	Year = {2012},
	Bdsk-Url-1 = {http://www.paraview.org}}
